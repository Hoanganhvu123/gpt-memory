{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tovana Memory üß† with LangGraph ü¶úüï∏Ô∏è: Cross-Thread Memory Sharing\n",
    "\n",
    "Welcome to this cookbook on integrating Tovana Memory with LangGraph! In this tutorial, we'll explore how to use Tovana's memory and belief system within a LangGraph agent, demonstrating cross-thread memory sharing using LangGraph's wider channel memory feature.\n",
    "\n",
    "This cookbook is based on the [LangGraph Wider Channels Memory Cookbook](https://github.com/langchain-ai/langgraph/blob/2a46c60551dd6baca9cbb02a7bb6effebb033d62/examples/memory/wider-channels.ipynb?short_path=8f86cb3), adapted to showcase Tovana's unique features.\n",
    "\n",
    "## What we'll cover:\n",
    "\n",
    "1. Installing required packages\n",
    "2. Setting up the environment\n",
    "3. Initializing Tovana MemoryManager\n",
    "4. Creating a LangGraph agent with Tovana memory\n",
    "5. Demonstrating cross-thread memory sharing\n",
    "6. Analyzing the results\n",
    "\n",
    "A key feature we'll demonstrate is batch message saving. Instead of saving every message individually, we'll save messages in batches. This approach preserves context, saves processing time and costs, and often leads to better results as it allows the system to understand the full context of a conversation before updating the memory.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing required packages\n",
    "\n",
    "First, let's install the necessary packages. Run the following cell to install LangChain, LangGraph, Tovana, and LangChain OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langgraph tovana langchain-openai\n",
    "\n",
    "print(\"Required packages installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the environment\n",
    "\n",
    "Now that we have the required packages installed, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Annotated, Any, Dict\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.graph import END, START\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.graph.state import StateGraph\n",
    "from langgraph.managed.shared_value import SharedValue\n",
    "from langgraph.store.memory import MemoryStore\n",
    "\n",
    "from memory import MemoryManager\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing Tovana MemoryManager\n",
    "\n",
    "Now, let's initialize the Tovana MemoryManager with our specific configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_manager = MemoryManager(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    provider=\"openai\",\n",
    "    temperature=0.2,\n",
    "    business_description=\"\"\"You are a top-rated personal chef renowned for transforming everyday dishes into gourmet culinary experiences.\n",
    "    Your specialty lies in elevating familiar comfort foods with sophisticated techniques,\n",
    "    high-quality ingredients, and artistic presentation.\n",
    "    Provide creative ideas and detailed instructions\n",
    "    for turning a classic [dish name] into an elegant, restaurant-quality meal.\"\"\",\n",
    "    include_beliefs=True,\n",
    ")\n",
    "\n",
    "print(\"Tovana MemoryManager initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a LangGraph agent with Tovana memory\n",
    "\n",
    "Let's create our LangGraph agent that incorporates Tovana memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(MessagesState):\n",
    "    user_id: str\n",
    "    info: Annotated[dict, SharedValue.on(\"user_id\")]\n",
    "\n",
    "prompt = \"\"\"You are helpful assistant.\n",
    "\n",
    "Here is what you know about the user:\n",
    "\n",
    "<info>\n",
    "{info}\n",
    "</info>\n",
    "\n",
    "Help out the user.\"\"\"\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "def call_model(state):\n",
    "    facts = [d[\"fact\"] for d in state[\"info\"].values()]\n",
    "    info = \"\\n\".join(facts)\n",
    "    system_msg = prompt.format(info=info)\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    print(f\"AI: {response.content}\")\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }\n",
    "\n",
    "def get_user_input(state: AgentState) -> Dict[str, Any]:\n",
    "    information_from_stdin = str(input(\"\\nHuman: \"))\n",
    "    return {\"messages\": [HumanMessage(content=information_from_stdin)]}\n",
    "\n",
    "def route(state):\n",
    "    num_human_input = sum(1 for message in state[\"messages\"] if message.type == \"human\")\n",
    "    if num_human_input < 5:\n",
    "        return \"not_enough_info\"\n",
    "    if num_human_input == 5:\n",
    "        return \"enough_user_input\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def update_memory(state):\n",
    "    memories = {}\n",
    "    # Batch update memory with all messages at once\n",
    "    memory_manager.batch_update_memory(state[\"user_id\"], state[\"messages\"])\n",
    "    memory_context = memory_manager.get_memory_context(user_id=state[\"user_id\"])\n",
    "    memories[str(uuid.uuid4())] = {\"fact\": memory_context}\n",
    "    print(f\"# Tovana memory saved.. \\n # {memory_context}\")\n",
    "    return {\"messages\": [(\"human\", \"memory saved\")], \"info\": memories}\n",
    "\n",
    "memory = MemorySaver()\n",
    "kv = MemoryStore()\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(call_model)\n",
    "graph.add_node(update_memory)\n",
    "graph.add_node(get_user_input)\n",
    "\n",
    "graph.add_edge(\"update_memory\", \"call_model\")\n",
    "graph.add_edge(START, \"get_user_input\")\n",
    "graph.add_edge(\"get_user_input\", \"call_model\")\n",
    "graph.add_conditional_edges(\n",
    "    \"call_model\",\n",
    "    route,\n",
    "    {\n",
    "        \"not_enough_info\": \"get_user_input\",\n",
    "        \"enough_user_input\": \"update_memory\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "graph = graph.compile(checkpointer=memory, store=kv)\n",
    "\n",
    "print(\"LangGraph agent with Tovana memory created.\")"
   ]
  },{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "### Visualizing the LangGraph Agent Flow\n",
  "\n",
  "The following image represents the flow of our LangGraph agent with Tovana memory:\n",
  "\n",
  "![LangGraph Tovana Flow](./static/langgraph_cooking_agent.png)\n",
  "\n",
  "This graph visualization shows the different states and transitions of our agent:\n",
  "\n",
  "- The process starts at the `_start_` node.\n",
  "- It then moves to `get_user_input` to collect information from the user.\n",
  "- The `call_model` node represents where the AI model processes the input and generates a response.\n",
  "- If there's not enough information, it loops back to `get_user_input` via the `not_enough_info` path.\n",
  "- When there's enough user input, it proceeds to `update_memory` via the `enough_user_input` path.\n",
  "- After updating the memory, it returns to `call_model` for further processing.\n",
  "- The process can end at any point if certain conditions are met, leading to the `_end_` node.\n",
  "\n",
  "This visual representation helps in understanding the flow of information and decision-making process in our LangGraph agent integrated with Tovana memory. It illustrates how the agent interacts with the user, processes information, updates memory, and makes decisions based on the amount of information gathered."
 ]
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstrating cross-thread memory sharing\n",
    "\n",
    "Now, let's demonstrate how memory can be shared across different threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = str(uuid.uuid4())\n",
    "\n",
    "print(\"Starting first thread...\")\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n",
    "res = graph.invoke({\"user_id\": user_id}, config=config)\n",
    "\n",
    "print(\"\\nStarting second thread...\")\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": user_id}}\n",
    "res2 = graph.invoke({\"user_id\": user_id}, config=config)\n",
    "\n",
    "print(\"\\nCross-thread memory sharing demonstration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing the results\n",
    "\n",
    "Let's analyze what happened in our demonstration:\n",
    "\n",
    "1. We created two separate threads (`thread_id` 1 and 2) for the same user (`user_id`).\n",
    "2. In each thread, the agent collected information from the user through multiple interactions.\n",
    "3. After gathering enough information (5 user inputs), Tovana's memory manager updated the user's memory using batch processing.\n",
    "4. The updated memory was then available in both threads, demonstrating cross-thread memory sharing.\n",
    "\n",
    "This approach allows for consistent user experiences across different conversation threads, leveraging Tovana's memory capabilities within the LangGraph framework. The batch processing of messages ensures that we capture the full context of the conversation before updating the memory, leading to more accurate and meaningful memory updates.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this cookbook, we've explored how to integrate Tovana Memory üß† with LangGraph ü¶úüï∏Ô∏è, demonstrating:\n",
    "\n",
    "- Initialization of Tovana MemoryManager\n",
    "- Creation of a LangGraph agent with Tovana memory\n",
    "- Cross-thread memory sharing using LangGraph's wider channel memory feature\n",
    "- Batch processing of messages for efficient and context-aware memory updates\n",
    "\n",
    "This integration enables more personalized and context-aware AI interactions, allowing your agents to maintain and utilize user-specific information across different conversation threads. The batch processing approach saves processing time and costs while potentially improving the quality of memory updates.\n",
    "\n",
    "Experiment with this setup to create even more sophisticated AI agents that can learn and adapt from user interactions over time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
